{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 1 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem just right after the words **BEGIN SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "* The are problems with \\* mark - they are not obligatory. You can get **EXTRA POINTS** for solving them.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (1 pt.): Information criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that regression model is\n",
    "$$y = \\sum_{i=1}^k \\beta_i x_i + \\varepsilon,$$\n",
    "and $\\varepsilon$ is dictributed normally: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, $\\sigma^2$ is known.\n",
    "\n",
    "Prove that the model with highest Akaike information criterion is the model with smallest Mallow's $C_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution** <br>\n",
    "We have a linear regression model with a white Gaussian noise $y = \\sum_{i=1}^k \\beta_i x_i + \\varepsilon,$ $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. Let's denote training sample $S_{m} = \\{(\\boldsymbol{x_i}, y_i)\\}, i = 1, \\dots, m$, $\\boldsymbol{x_i} \\in \\mathbb{R}^d, y_i \\in \\mathbb{R}^{1}$. $\\boldsymbol{X} = \\{\\boldsymbol{x_i}, i = 1, \\dots, m\\} \\in \\mathbb{R}^{m \\times d}$  is a design matrix. <br>\n",
    "Let $J \\in \\{1, \\dots, d\\}$ is a subset of features from $\\boldsymbol{x}$ we use to construct a linear model. Let's denote a regression function as $\\hat{f}_{J}(\\boldsymbol{x})= \\boldsymbol{\\hat{\\beta}}_{J}\\boldsymbol{x}_{J}^{\\top}$, where $\\boldsymbol{\\hat{\\beta}_{J}}$ are estimates of coefficients $\\boldsymbol{\\beta}_{J}$, corresponding to $\\boldsymbol{X}_{J}$ (submatrix of $\\boldsymbol{X}$ with features from $J$). $\\hat{y}_i(J) = \\hat{f}_{J}(\\boldsymbol{x_i})$ is a prediction. So, risk estimate on the training set is equal to \n",
    "$$\n",
    "    \\hat{R}_{tr}(J) = \\frac{1}{m}\\sum\\limits_{i = 1}^{m} (\\hat{y}_i(J) - y_i)^2,\n",
    "$$\n",
    "that is a biased risk estimate (since $\\hat{y}_i(J)$ and $y_i$ are correlated (explained in Lecture 7)). <br>\n",
    "Using newly obtained output measurements $y^{*} = \\sum_{i=1}^k \\beta_i x_i + \\varepsilon^{*},$ $\\varepsilon^{*} \\sim \\mathcal{N}(0, \\sigma^2)$, we can write real generalization error of a prediction (in-sample) error\n",
    "$$\n",
    "    {R}(J) = \\frac{1}{m}\\sum\\limits_{i = 1}^{m} \\mathbb{E}(\\hat{y}_i(J) - y_i^{*})^2\n",
    "$$\n",
    "Using results from Lecture 7 (definition of Mallow's $C_{p}$ in the linear case) for linear regression we get $C_p$ Mallow statistics, representing asymptotically unbiased estimate of the regression risk\n",
    "$$\n",
    "    \\hat{R}(J) = \\hat{R}_{tr}(J) + \\frac{2 \\sigma^2}{m}|J|\n",
    "$$\n",
    "To select a subset of features $J$ we should minimize greedily\n",
    "$$\n",
    "    \\hat{R}(J) = \\hat{R}_{tr}(J) + \\frac{2 \\sigma^2}{m}|J| \\to \\min\\limits_{\\boldsymbol{\\beta}_{J}, J}\n",
    "$$\n",
    "Now let's consider Akaike Information Criterion (AIC)\n",
    "$$\n",
    "    \\mathcal{L} - |J| \\to \\max\\limits_{\\boldsymbol{\\beta}_{J}, J},\n",
    "$$\n",
    "$\\mathcal{L}$ is a model log-likelihood, $|J|$ is a number of free model parameters. <br>\n",
    "For linear regression model with Gaussian noise Log-likelihood of sample $S_{m}$ has the form\n",
    "$$\n",
    "    \\mathcal{L}_{J}(\\boldsymbol{\\beta}) = m\\log\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg) - \\frac{1}{2\\sigma^{2}}\\sum\\limits_{i = 1}^{m}(y_i - \\boldsymbol{\\beta_{J}}\\boldsymbol{x_{i, J}^{\\top}})^{2}\n",
    "$$\n",
    "Note that the second component of this expression can be represented as \n",
    "$$\n",
    "- \\frac{1}{2\\sigma^{2}}\\sum\\limits_{i = 1}^{m}(y_i - \\boldsymbol{\\beta_{J}}\\boldsymbol{x_{i, J}^{\\top}})^{2} = -\\frac{m}{2\\sigma^2}\\hat{R}_{tr}(J)\n",
    "$$\n",
    "So, we can write AIC\n",
    "$$\n",
    "    \\mathcal{L}_{J}(\\boldsymbol{\\beta}) - |J| = m\\log\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg) -  \\frac{1}{2\\sigma^{2}}\\sum\\limits_{i = 1}^{m}(y_i - \\boldsymbol{\\beta_{J}}\\boldsymbol{x_{i, J}^{\\top}})^{2} - |J| \\to \\max\\limits_{\\boldsymbol{\\beta}_{J}, J}\n",
    "$$\n",
    "Since $\\sigma$ is known, we will get\n",
    "$$\n",
    "    \\mathcal{L}_{J}(\\boldsymbol{\\beta}) - |J| = -\\frac{m}{2\\sigma^2}\\hat{R}_{tr}(J) - |J| + const \\to \\max\\limits_{\\boldsymbol{\\beta}_{J}, J}\n",
    "$$\n",
    "So, we get that\n",
    "$$\n",
    "    -\\frac{2\\sigma^2}{m}(\\mathcal{L}_{J} - |J|) \\boldsymbol{\\sim} \\hat{R}_{tr}(J) + \\frac{2\\sigma^2}{m}|J|\n",
    "$$\n",
    "It means that we proved that in the model with highest Akaike information criterion is the model with smallest Mallow's $C_{p}$ because from the previous expression we get\n",
    "$$\n",
    "    \\mathcal{L}_{J}(\\boldsymbol{\\beta}) - |J| \\to \\max\\limits_{\\boldsymbol{\\beta}_{J}, J} \\Longleftrightarrow \\hat{R}(J) = \\hat{R}_{tr}(J) + \\frac{2 \\sigma^2}{m}|J| \\to \\min\\limits_{\\boldsymbol{\\beta}_{J}, J}\n",
    "$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: gradient boosting, adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting and its theory\n",
    "\n",
    "Minimization of the loss function is an optimization task, and \"Gradient Boosting\"\n",
    "is one of the many methods to perform optimization. It shoould be noted that it\n",
    "uses *greedy* approach and therefore, like greedy algorithms in CS, may produce\n",
    "results that are not *globally* optimal.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & b_n(x) := \\text{the best base model from the family of the algorithms $\\mathcal{A}$} \\\\\n",
    "    & \\gamma_n(x) := \\text{scale or weight of the new model} \\\\\n",
    "    & a_N(x) = \\sum_{n=0}^N \\gamma_n b_n(x) := \\text{the final composite model}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Algorithm\n",
    "\n",
    "Consider a loss loss function $L(y, z)$ for target $y$ and prediction $z$, and let\n",
    "$(x_i, y_i)_{i=1}^l$ be our train dataset for a regression task. \n",
    "\n",
    "\n",
    "1. Initialize $a_0(x) = \\hat{z}$ with the *flat constant prediction*\n",
    "$\\hat{z} = \\arg\\min\\limits_{z \\in \\mathbb{R}} \\sum_{i=1}^l L(y_i, z)$;\n",
    "2. For $n$ from `1` to `n_boost_steps` do:\n",
    "    * Solve the current subporblem $G_n(b_n, \\gamma_n) \\to \\min\\limits_{b_{n}, \\gamma_n}$\n",
    "    where \n",
    "    $$ G_n(b, \\gamma) = \\sum_{i=1}^l L\\bigl(y_i, a_{n-1}(x_i) + \\gamma b(x_i)\\bigr) $$\n",
    "    with the following method:\n",
    "    \\begin{align}\n",
    "      & s_i = - \\frac{\\partial}{\\partial z} L(y_i, z) \\Big\\vert_{z=a_{n-1}(x_i)}\n",
    "          \\\\\n",
    "      & b_n(x) = \\arg\\min\\limits_{b\\in\\mathcal{A}}\\sum_{i=1}^l \\bigl(b(x_i) - s_i\\bigr)^2\n",
    "          \\\\\n",
    "      & \\gamma_n = \\arg\\min_\\gamma G_n(b_n, \\gamma)\n",
    "          \\\\\n",
    "      & a_n(x) = a_{n-1}(x) + \\gamma_n b_n(x)\n",
    "    \\end{align}\n",
    "3. return $a_N(x) = a_0(x) + \\sum_{n=1}^N \\gamma_n b_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (1 pt.)\n",
    "\n",
    "At the $n$-th step of Garient Boosting ($n \\geq 1$) we compute the \"residuals\"\n",
    "$(s_i)_{i=1}^l$ and learn $x\\mapsto b_n(x)$ with a regression algorithm $\\mathcal{A}$\n",
    "applied to the dataset $(x_i, s_i)_{i=1}^l$. For the next two tasks **assume\n",
    "that we have already perfomed these substeps**.\n",
    "\n",
    "Derive the **optimal value** of $\\gamma_n$ for *MSE* loss function $L(y, z) = \\tfrac12 (y - z)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution** <br>\n",
    "Since we have already done sebsteps with derivation of $(s_i)_{i=1}^l$ and $b_n(x)$, we just need to solve optimization problem\n",
    "\n",
    "$$\n",
    "    \\gamma_n = \\arg\\min_\\gamma G_n(b_n, \\gamma),\n",
    "$$\n",
    "\n",
    "where $ G_n(b, \\gamma) = \\sum_{i=1}^l L\\bigl(y_i, a_{n-1}(x_i) + \\gamma b(x_i)\\bigr) $ and $L(y, z) = \\tfrac12 (y - z)^2$. <br>\n",
    "\n",
    "Let's solve this problem\n",
    "$$\n",
    "    \\frac{\\partial{G_n(b_n, \\gamma)}}{\\partial{\\gamma}} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{G_n(b_n, \\gamma)}}{\\partial{\\gamma}} = \\frac{\\partial}{\\partial{\\gamma}}\\bigg\\lbrack\\sum\\limits_{i = 1}^{l}\\frac{1}{2}\\big(y_{i} - \\big(a_{n-1}(x_i) + \\gamma b_{n}(x_{i}) \\big)\\big)^{2}\\bigg\\rbrack\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{G_n(b_n, \\gamma)}}{\\partial{\\gamma}} = \\sum\\limits_{i = 1}^{l} \\bigg(\\big(y_{i} - \\big(a_{n-1}(x_i) + \\gamma b_{n}(x_{i}) \\big) \\big)\\big( -\\frac{\\partial{(\\gamma b_{n}(x_{i}))}}{\\partial{\\gamma}}\\big)\\bigg) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{G_n(b_n, \\gamma)}}{\\partial{\\gamma}} = \\sum\\limits_{i = 1}^{l} \\bigg(\\big( y_{i} - \\big(a_{n-1}(x_{i}) + \\gamma b_{n}(x_i) \\big)\\big)\\big(-b_{n}(x_{i}) \\big)\\bigg) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\gamma_{opt} = \\frac{\\sum\\limits_{i = 1}^{l}\\big(y_{i} b_{n}(x_{i}) - a_{n-1}(x_{i}) b_{n}(x_{i}) \\big)}{\\sum\\limits_{i = 1}^{l}b_{n}^{2}(x_{i})}\n",
    "$$\n",
    "\n",
    "So, we have found the optimal value of $\\gamma_{n}$\n",
    "$$\n",
    "    \\gamma_{n} = \\gamma_{opt} = \\frac{\\sum\\limits_{i = 1}^{l}\\bigg(\\big(y_{i} - a_{n-1}(x_{i})\\big)b_{n}(x_{i}) \\bigg)}{\\sum\\limits_{i = 1}^{l}b_{n}^{2}(x_{i})}\n",
    "$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (1+1+1 pt.)\n",
    "\n",
    "Let $S = (x_i, y_i)_{i=1}^l$ be a sample for a classification task $y_i \\in \\{-1, +1\\}$.\n",
    "\n",
    "The **AdaBoost** algorithm can be regarded as a gradient boosting algorithm\n",
    "with the exponential loss $L(y,z) = e^{-y z}$. Consider the state of **AdaBoost**\n",
    "at the $T$-step\n",
    "$$ G_{T}(b, \\gamma)\n",
    "    = \\sum_{i=1}^l L\\bigl(y_i, a_{T-1}(x_i) + \\gamma b(x_i)\\bigr)\n",
    "    = \\sum_{i=1}^l \\underbrace{\\exp(- y_i a_{T-1}(x_i))}_{w^{T-1}_i}\n",
    "        \\exp(- y_i \\gamma b(x_i))\n",
    "    \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.1 (1 pt.)\n",
    "\n",
    "Derive the next weights $w^T_i$ used in $G_T$ at the stage $T$ as a function\n",
    "of the learnt classifier $b_T$ and the current weights $w^{T-1}_i$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution <br>\n",
    "Let's consider $w_{i}^{T} = \\exp{(-y_{i}a_{T}(x_{i}))}$. We need to find $a_{T}(x_{i})$:\n",
    "\n",
    "$$\n",
    "    a_{T}(x_{i}) = a_{T-1}(x_{i}) + \\gamma_{T}b_{t}(x_{i})\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w_{i}^{T-1} = \\exp{(-y_{i}a_{T-1}(x_{i}))}\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "    a_{T-1}(x_{i}) = -\\frac{\\ln(w_{i}^{T-1})}{y_{i}}\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "    a_{T}(x_{i}) = -\\frac{\\ln(w_{i}^{T-1})}{y_{i}} + \\gamma_{T}b_{T}(x_{i})\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w_{i}^{T} = \\exp{\\big(\\ln(w_{i}^{T-1}) - y_{i}\\gamma_{T}b_{T}(x_{i})\\big)} = w_{i}^{T-1}\\exp(-y_{i}\\gamma_{T}b_{T}(x_{i}))\n",
    "$$\n",
    "\n",
    "Let's denote $\\tilde{w_{i}^{T}} = \\frac{w_{i}^{T-1}}{\\sum\\limits_{j}\\big(w_{j}^{T-1}\\big)}$. <br>\n",
    "And let's use some results from the lecture 8 (AdaBoost). It was shown on the lecture that in minimization problem\n",
    "$ G_{T}(b_{T}, \\gamma_{T}) = \\sum_{i=1}^l w_{i}^{T-1}\\exp(- y_i \\gamma_{T} b_T(x_i)) \\to \\min\\limits_{\\gamma_{T}}$ for fixed $b_{T}$ (as in this problem) the optimal value of $\\gamma_{T}$ is\n",
    "$$\n",
    "    \\gamma_{T} = \\gamma_{opt} = \\frac{1}{2}\\ln{\\big(\\frac{P_{T}}{N_{T}}\\big)} = \\frac{1}{2}\\ln{\\big(\\frac{1 - N_{T}}{N_{T}}\\big)},\n",
    "$$\n",
    "where $N_{T} = N_{T}(b_{T}, \\boldsymbol{\\tilde{w_{T}}}) = \\sum\\limits_{i = 1}^{l}\\big(\\tilde{w_{i}^{T}}\\cdot \\mathbb{1}_{\\{y_{i}b_{T}(x_{i}) \\le 0\\}} \\big)$ is a weighted classification error. $P_{T} = 1 - N_{T}$, $\\boldsymbol{\\tilde{w_{T}}} = (\\tilde{w_{1}^{T}}, \\dots, \\tilde{w_{l}^{T}})$\n",
    "So, we will get\n",
    "\n",
    "$$\n",
    "   w_{i}^{T} = w_{i}^{T-1}\\exp{\\bigg(-\\frac{y_{i}b_{T}(x_{i})}{2}\\ln{\\Bigg\\lbrack \\frac{1 - \\sum\\limits_{i = 1}^{l}\\big(\\tilde{w_{i}^{T}}\\cdot \\mathbb{1}_{\\{y_{i}b_{T}(x_{i}) \\le 0\\}} \\big)}{\\sum\\limits_{i = 1}^{l}\\big(\\tilde{w_{i}^{T}}\\cdot \\mathbb{1}_{\\{y_{i}b_{T}(x_{i}) \\le 0\\}} \\big)}\\Bigg\\rbrack} \\bigg)},\n",
    "$$\n",
    "\n",
    "where $\\tilde{w_{i}^{T}} = \\frac{w_{i}^{T-1}}{\\sum\\limits_{j}\\big(w_{j}^{T-1}\\big)}$. <br>\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2 (1 pt.)\n",
    "\n",
    "Compute the ratio of weights $(w^T_i)_{i=1}^l$ between the normal and outlier\n",
    "samples in $S$. Propose a **formal definition of being an outlier**, and reflect\n",
    "on the value of *margin* for both.\n",
    "\n",
    "<span style=\"color:green\">**HINT**</span>: *margin* value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution <br>\n",
    "\n",
    "An outlier is an observation which deviates very much from the other observations in the dataset.<br>\n",
    "So, it is obvious that in the most cases outliers will be misclassified. To clarify this we can imagine that there is a noised observation (that is an outlier), then the classifier will not be able to predict the correct label because the label for noised observation is just a kind of a random value. Due to this fact we are able to write a definition of an outlier from the first sentence in the terms of a margin value. If $x_{i}$ is a feature vector, $y_{i}$ is a true value and $\\hat{h(x_{i})}$ is a predicted value, then the margin is expressed as $\\rho = y_{i} \\cdot \\hat{h}(x_{i})$. For misclassified points the margin will be non-positive since the true class label and the prediction will have different signs (or one of them will be zero and one non-zero if the labels are from $\\{0, 1\\}$).<br>\n",
    "So, summarizing, definition of an outlier will be as follows\n",
    "\n",
    "$$\n",
    "    (x_{i}, y_{i}) \\mbox{ - an outlier} \\Rightarrow y_{i}\\cdot\\sum\\limits_{t=1}^{T}(b_{t}(x_{i})\\gamma_{t}) \\le 0\n",
    "$$\n",
    "In this definition we used the classifier $\\sum\\limits_{t=1}^{T}(b_{t}(x_{i})\\gamma_{t})$ obtained via AdaBoost. <br>\n",
    "Now let's denote $(x_{i}, y_{i})$ is a normal observation sample, $(x_{j}, y_{j})$ is an outlier. Using notation of $w_{i}^{T}$ that is  $w_{i}^{T} = \\exp{\\big( -y_{i}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{i}))\\big)}$ we will get\n",
    "$$\n",
    "    \\frac{w_{i}^{T}}{w_{j}^{T}} = \\frac{\\exp{\\big( -y_{i}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{i}))\\big)}}{\\exp{\\big( -y_{j}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{j}))\\big)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{w_{i}^{T}}{w_{j}^{T}} = \\exp{\\bigg\\lbrack -y_{i}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{i})) + y_{j}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{j}))\\bigg\\rbrack}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3 (1 pt.)\n",
    "\n",
    "Conclude about **sensitivity** of Adaboost to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "Let's consider the expression from the previous point\n",
    "$$\n",
    "    \\frac{w_{i}^{T}}{w_{j}^{T}} = \\exp{\\bigg\\lbrack -y_{i}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{i})) + y_{j}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{j}))\\bigg\\rbrack}\n",
    "$$\n",
    "\n",
    "Using the definition of an outlier in terms of margins we will get $\\bigg\\lbrack -y_{i}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{i})) + y_{j}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{j}))\\bigg\\rbrack \\le 0$. <br>\n",
    "Let's denote $\\theta = \\bigg\\lbrack -y_{i}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{i})) + y_{j}\\sum\\limits_{t=1}^{T}(\\gamma_{t}b_{t}(x_{j}))\\bigg\\rbrack < 0$. Moreover, in the most cases $\\theta < 0$ since for normal samples AdaBoost will give the predicton with high accuracy. So, we get\n",
    "\n",
    "$$\n",
    "    \\frac{w_{i}^{T}}{w_{j}^{T}} = \\exp{\\theta} < 1\n",
    "$$\n",
    "\n",
    "So, weights for outliers will be greater than for normal sample points. Moreover, from the AdaBoost algorithm's steps for $w_{i}^{t}$ $w_{i}^{t+1} = w_{i}^{t} \\cdot \\exp{(-\\gamma_{t}y_{i}b_{t}(x_{i}))}$ we can see that for miclassified points weights for the next iteratons $w_{i}^{t+1}$ will increase (since $\\gamma_{t}$ decreases). So, classifiers on the next iteratons will assign greater weights to data points that were misclassified in the previous iteration. So, AdaBoost is sensitive to outliers in the way that was explained above.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (2+1+2 pt.): Alternative objective functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem studies boosting-type algorithms defined with objective\n",
    "functions different from that of AdaBoost.We assume that the training\n",
    "data are given as m labeled examples $(x_{1},y_{1}),...,(x_{m},y_{m}) \\in X \\times \\{-1,+1\\}$.We further assume that $\\Phi$ is a strictly increasing convex and differentiable function over $\\mathbb{R}$ such that:$\\ \\forall x \\geqslant 0,\\Phi(x)\\geqslant 1 \\ and \\ \\forall x < 0,\\Phi(x) > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.1 (2 pt.)\n",
    "Consider the loss function $L(a) =\\sum_{i=1}^{m}\\Phi \\left( -y_{i}g(x_i) \\right)$ where $g$ is a linear combination of base classifiers, i.e., $g= \\sum_{t=1}^{T} a_t h_t$(as in\n",
    "AdaBoost). Derive a new boosting algorithm using the objective function $L$. In particular, characterize the best base classifier $h_u$ to select at each round of boosting if we use coordinate descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2 (1 pt.)\n",
    "Consider the following functions:  \n",
    "\n",
    "1. zero-one loss $\\Phi_1(−u) = 1_{u\\leqslant0}$;  \n",
    "2. least squared loss $\\Phi_2(−u) = (1 − u)^2$;  \n",
    "3. SVM loss $\\Phi_3(−u) = \\max\\{0, 1 − u\\}$;  \n",
    "4. logistic loss $\\Phi_4(−u) = \\log(1 + e^{−u})$.  \n",
    "\n",
    "Which functions satisfy the assumptions on $\\Phi$ stated earlier in this\n",
    "problem?\n",
    "\n",
    "Compute the gradient for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3* (2 pt.)\n",
    "For each loss function satisfying the assumptions in Task 5 formualtion, derive the\n",
    "corresponding boosting algorithm. How do the algorithm(s) differ\n",
    "from AdaBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. (1 pt.)\n",
    "Consider a two-layer network function of the form\n",
    "    \\begin{equation}\n",
    "    y_k(x, \\mathbf{w}) = \\sigma \\left ( \\sum_{j=1}^M w_{kj}^{(2)} \\sigma \\left(\\sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)}\\right)\n",
    "                               + w_{k0}^{(2)}\\right)\n",
    "    \\end{equation}\n",
    "in which the nonlinear activation functions are logistic sigmoid functions\n",
    "    \\begin{equation}\n",
    "    \\sigma(a) = (1 + \\exp(−a))^{-1}.\n",
    "    \\end{equation}\n",
    "Show that there exists an equivalent network, which computes exactly the same function but with hidden unit activation function given by hyperbolic tangent ${\\rm tanh}(a)$\n",
    "    \\begin{equation}\n",
    "    {\\rm tanh}(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}.\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6*. Data augmentation (2 pt.)\n",
    "One way to encourage invariance of a model w.r.t a set of transformations is to expand the training set using transformed versions of the original input patterns.\n",
    "Consider the framework for training with transformed data in the special case when the transformation is simply addition of random noise $x \\rightarrow x + \\xi$ where $\\xi$ has a Gaussian distribution with zero mean and unit variance. The error function for untransformed inputs can be written (in the infinite data set limit) in the form\n",
    "    \\begin{equation}\n",
    "    E = \\frac12 \\int \\int (y(\\mathbf{x}) - t)^2 p(t|\\mathbf{x}) p(\\mathbf{x}){\\rm d}\\mathbf{x} {\\rm d}t.\n",
    "    \\end{equation}\n",
    "If we now consider an infinite number of copies of each data point perturbed by the transformation, then the error function can be written as\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} = \\frac12 \\int\\int(y(x + \\xi) - t)^2p(t | \\mathbf{x})p(\\mathbf{x}) p(\\xi){\\rm d}\\mathbf{x}{\\rm d}t {\\rm d}\\xi\n",
    "    \\end{equation}\n",
    "Using Taylor series expansion of $y(\\mathbf{x} + \\xi)$ show that\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} \\simeq E + \\lambda \\Omega\n",
    "    \\end{equation}\n",
    "where $\\Omega$ is a regularizer which takes the form of Tikhonov regularizer\n",
    "    \\begin{equation}\n",
    "    \\Omega = \\frac12 \\int \\|\\nabla y(\\mathbf{x})\\|^2 p(\\mathbf{x}){\\rm d} \\mathbf{x}.\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
