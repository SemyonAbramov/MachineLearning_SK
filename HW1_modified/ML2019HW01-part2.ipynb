{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem just right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "* The are two problems with \\* mark - they are not obligatory. You can get **EXTRA POINTS** for solving them.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Ridge Regression (1 point)\n",
    "Let us consider the problem of linear ridge regression for data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{d\\times 1}$. Let the objects have positive **sample weights** $v_{i}>0$, i.e. the optimization problem is\n",
    "$$\\sum_{i=1}^{n}v_{i}\\cdot L(y_{i}, \\hat{y}_{i})+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}=\\sum_{i=1}^{n}v_{i}\\cdot (\\langle\\boldsymbol{w},\\boldsymbol{x}_{i}\\rangle-y_{i})^{2}+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}\\rightarrow \\min_{\\boldsymbol{w}}.$$\n",
    "This problem reduces to classical linear ridge regression when $v_{i}\\equiv 1$. The matrix form of the problem is\n",
    "$$(Xw-y)^{\\top}V(Xw-y)+\\frac{\\lambda}{2}w^{\\top}w\\rightarrow\\min_{w},$$\n",
    "where $V=V^{\\top}\\in\\mathbb{R}^{n\\times n}$ is the diagonal matrix with diagonal elements $v_{1},\\dots, v_{n}$. Note that the quadratic problem is still convex (w.r.t. $\\boldsymbol{w}$), thus, the solution is unique. Solve this problem for any (symmetric) positive-definite matrix $V$ (not just diagonal) and provide the answer in the matrix form.\n",
    "### Your solution:\n",
    "Chain rule for our case:\n",
    "$$\n",
    "\\frac{\\partial f(w)}{\\partial w} = \\frac{\\partial u}{\\partial w} \\frac{\\partial f(w)}{\\partial u}\n",
    "$$ \n",
    "in our case $u$ is a vector $u = Xw - y$.\n",
    "Get expression\n",
    "$$\n",
    "    \\frac{\\partial f(w)}{\\partial w} = \\frac{\\partial u^{\\top}Vu}{\\partial u}\n",
    "$$\n",
    "where $u=u(w)$. <br>\n",
    "We will use next calculus derivations\n",
    "$$\n",
    "    \\frac{\\partial x^{\\top}Vx}{\\partial x} = Vx + V^{\\top}x,\n",
    "$$\n",
    "for $V$ is not a function of $x$.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial (Xw - y)}{\\partial w} = X^{\\top}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial (w^{\\top}w)}{\\partial w} = 2w,\n",
    "$$\n",
    "note, that we are using denominator form.<br>\n",
    "So, let's solve the equation\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial((Xw-y)^{\\top}V(Xw-y)+\\frac{\\lambda}{2}w^{\\top}w)}{\\partial w} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "    X^{\\top}(V(Xw - y) + V^{\\top}(Xw - y)) + \\lambda w = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "    X^{\\top}VXw + X^{\\top}V^{\\top}Xw + \\lambda w = (X^{\\top}V + X^{\\top}V^{\\top})y\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w = (X^{\\top}(V + V^{\\top})X + \\lambda I)^{-1}X^{\\top}(V + V^{\\top})y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Logistic Regression (1 point)\n",
    "Let us consider the case when in the problem of binary classification the training set is lineary separable. Show that in this case the optimization problem for logistic regression **without L2-regularization** does not have optimum.\n",
    "### Your solution:\n",
    "Let's consider Logistic Regression without regularization: $ L(w) = \\sum\\limits_{i=1}^m \\log(1 + \\exp(-(\\boldsymbol{w^\\top} \\boldsymbol{x_i})y_i))$. If the training set is linearly separable, it means we can divide objects of two classes by hyperplane $\\boldsymbol{w^\\top} \\boldsymbol{x} = 0$. It means that any $\\boldsymbol{\\tilde{w}} = \\gamma \\boldsymbol{{w}}, \\gamma \\in \\mathbb{R}$ will give such separating hyperplane, so, there will be an infinite number of solutions and $\\|\\tilde{w}\\| \\to \\infty , \\gamma \\to \\infty$. Consequently, there is no optimal solution for considered optimization problem. Such issue will degenerate sigmoid function $\\sigma(\\boldsymbol{w^\\top x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{w^\\top x})}$ to step function (because $\\log(Y|X) \\to 1$ when $\\gamma \\to \\infty$, so $P \\to 1$), in other words overfitting will occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Bayesian Naive Classifier (1 point)\n",
    "Show that in case of $d$ binary-valued features $x_{j}\\in\\{0, 1\\}$ (for $j=1,2,\\dots,d$) Bayesian Naive Classifier's decision rule is linear.\n",
    "### Your solution:\n",
    "Let's consider fetaure vectors $\\boldsymbol{x}=(x_1, x_2, \\dots, x_d)^\\top$ with binary features $x_{j}\\in\\{0, 1\\}$ (for $j=1,2,\\dots,d$). Naive Bayes Classifier has maximum a posteriory decision rule $\\tilde{y} = \\arg\\max\\limits_{k \\in 1, \\dots, K} \\mathcal{p(C_k)}\\prod_{i=1}^d \\mathcal{p(x_i|C_k)}$. \\\\\n",
    "Let's consider when the classifier will predict class with label $c = 1$. It will occur if $P(c=1|\\boldsymbol{x}) \\ge P(c=0|\\boldsymbol{x})$, or\n",
    "$$\n",
    "    \\frac{P(\\boldsymbol{x}|c=1)P(c=1)}{P(\\boldsymbol{x}|c=0)P(c=0)} \\ge 1\n",
    "$$\n",
    "Using \"naive\" conditional independence $p(x_i|x_{i+1}, \\dots, x_d, C_k) = p(x_i|C_k)$, or $P(\\boldsymbol{x}|c) = \\prod_{i=1}^d P(x_i|c) $, we can write\n",
    "$$\n",
    "    \\frac{P(c=1)}{P(c=0)} \\prod_{i=1}^d \\frac{P(x_i|c=1)}{P(x_i|c=0)} \\ge 1\n",
    "$$\n",
    "Let's denote probabilities as follow $P(x_i|c=1)$ as $r_i^{x_i}(1 - r_i)^{(1-x_i)}$, $P(x_i|c=0)$ as $z_i^{x_i}(1 - z_i)^{(1-x_i)}$, $P(c=1)$ as $p$. In this case we will have next notations: $P(x_i=1|c=1)$ as $r_i$, $P(x_i=0|c=1)$ as $(1 - r_i)$, $P(x_i=1|c=0)$ as $z_i$, $P(x_i=0|c=0)$ as $(1 - z_i)$.\\\\\n",
    "Now we can write\n",
    "$$\n",
    "    \\frac{p}{1-p} \\prod_{i=1}^d \\frac{r_i^{x_i}(1 - r_i)^{(1-x_i)}}{z_i^{x_i}(1 - z_i)^{(1-x_i)}} \\ge 1\n",
    "$$\n",
    "$$\n",
    "    \\bigg( \\frac{p}{1-p} \\prod_{i=1}^d \\frac{(1 - r_i)}{(1 - z_i)} \\bigg) \\prod_{i=1}^d \\bigg(\\frac{(1 - z_i)}{(1 - r_i)} \\frac{r_i}{z_i}\\bigg)^{x_i} \\ge 1\n",
    "$$\n",
    "$$\n",
    "    \\log \\bigg( \\frac{p}{1-p} \\prod_{i=1}^d \\frac{(1 - r_i)}{(1 - z_i)} \\bigg) + \\sum_{i=1}^d x_i \\log\\bigg(\\frac{(1 - z_i)}{(1 - r_i)} \\frac{r_i}{z_i}\\bigg)\\ge 0\n",
    "$$\n",
    "Since expression $\\log \\bigg( \\frac{p}{1-p} \\prod_{i=1}^d \\frac{(1 - r_i)}{(1 - z_i)} \\bigg)$ is a constant, we can denote it as $b$. Than, denoting $\\log\\bigg(\\frac{(1 - z_i)}{(1 - r_i)} \\frac{r_i}{z_i}\\bigg)$ as $w_i$, we will get expression that predicts that $c = 1$ in such form\n",
    "$$\n",
    "    \\sum_{i=1}^d x_i w_i + b \\ge 0\n",
    "$$\n",
    "That is exactly equation for linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg)\\bigg],$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability of object $x_{i}$ to be present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:\n",
    "There is an original sample that contains $n$ objects ${x_1, x_2, \\dots, x_n}$. To get $\\hat{X}^{n}$ we will draw objects from  $X^{n}$ uniformly at random with replacement. Consequently, the probabilty to choose any object $x_i$ at any draw is $\\frac{1}{n}$. So, the probability not to choose this object is $1 - \\frac{1}{n}$. According to bootstrap approach we must choose objects from $X^{n}$ to $\\hat{X}^{n}$ $n$ times. It means, that after $n$ draws the probability not to choose $x_i$ will be $(1 - \\frac{1}{n})^{n}$. So, the probability to choose $x_i$ is equal to $\\bigg(1 - (1 - \\frac{1}{n})^{n}\\bigg)$.\\\\\n",
    "Now let's compute the limit of the probability for $n\\rightarrow\\infty$.\\\\\n",
    "Let's prove that $\\lim\\limits_{n \\to \\infty} (1 + \\frac{x}{n})^n = e^x$.\n",
    "$$\n",
    "    (1 + \\frac{x}{n})^n = e^{\\ln(1 + \\frac{x}{n})^n} = e^{n\\ln(1 + \\frac{x}{n})}\n",
    "$$\n",
    "$$\n",
    "    \\lim\\limits_{n \\to \\infty} (1 + \\frac{x}{n})^n = \\lim\\limits_{n \\to \\infty} e^{n\\ln(1 + \\frac{x}{n})}\n",
    "$$\n",
    "So, we need to find limit of exponent power,\n",
    "$$\n",
    "    \\lim\\limits_{n \\to \\infty}n\\ln(1 + \\frac{x}{n}) = \\lim\\limits_{n \\to \\infty} \\frac{\\ln(1 + \\frac{x}{n})}{\\frac{1}{n}}\n",
    "$$\n",
    "Using L'Hopital's Rule,\n",
    "$$\n",
    "    \\lim\\limits_{n \\to \\infty} \\frac{(\\frac{-x}{n^2})\\frac{1}{1+\\frac{x}{n}}}{-\\frac{1}{n^2}} = \\lim\\limits_{n \\to \\infty} \\frac{x}{1+\\frac{x}{n}} = x\n",
    "$$\n",
    "Proved\n",
    "$$\n",
    "     \\lim\\limits_{n \\to \\infty} (1 + \\frac{x}{n})^n = \\lim\\limits_{n \\to \\infty} e^{n\\ln(1 + \\frac{x}{n})} = e^x\n",
    "$$\n",
    "\n",
    "For $x = -1$ get $ \\lim\\limits_{n \\to \\infty}(1 - \\frac{1}{n})^{n} = e^{-1} \\approx 0,368$ <br>\n",
    "Finally, $\\lim\\limits_{n \\to \\infty} \\bigg(1 - (1 - \\frac{1}{n})^{n}\\bigg) = 1 - e^{-1} \\approx 0,632$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1+1=3 points)\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample, i.e. $\\frac{1}{n}\\sum_{i=1}^{n}L(y_{i}, \\hat{y})\\rightarrow\\min$. We consider three cases:\n",
    "1. Regression tree ($y_{i}\\in\\mathbb{R}$), absolute loss function $L(y,\\hat{y})=|y-\\hat{y}|$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-\\hat{y}|$ is the median of labels: \n",
    "$$\\hat{y}=\\text{median}(y_{1},\\dots,y_{n}).$$\n",
    "In this case, for simplicity you may assume that $n$ is even (or odd, as you wish).\n",
    "2. Regression tree ($y_{i}\\in\\mathbb{R}$), squared loss function $L(y,\\hat{y})=(y-\\hat{y})^{2}$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y})^{2}$ is the mean of labels:\n",
    "$$\\hat{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i.$$\n",
    "3. Classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$), zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq\\hat{y}]$ is the most frequent label:\n",
    "$$\\hat{y}=\\underset{k=1,2,\\dots,K}{\\operatorname{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=k].$$\n",
    "In this case, for simplicity you may assume that there is only one most frequent label.\n",
    "\n",
    "Suppose that that instead of using optimal prediction for this leaf we output the label from $y_{1},\\dots,y_{n}$ at random. What will happen with the (expected) loss on the training sample (will it increase/decrease/not change)? Prove your answer (separately for every case).\n",
    "### Your solution:\n",
    "1. Let's consider optimization mentioned in the task optimization problem: $\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-\\hat{y}| \\to \\min\\limits_{\\hat{y}}$. The optimal prediction is $\\hat{y}=\\text{median}(y_{1},\\dots,y_{n})$ (let's assume that $n$ is odd). So, the loss for the optimal solution is\n",
    "$$\n",
    "  L_{optimal} = \\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-\\hat{y}|,\n",
    "$$\n",
    "where notation $\\hat{y}$ is used for median.<br>\n",
    "Now let's show expected loss in the case when prediction is chosen at random.<br>\n",
    "We have\n",
    "$$\n",
    "    E(L^{*}) = \\frac{1}{n}\\sum_{j=1}^{n}\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-y_{j}|\n",
    "$$\n",
    "Let's consider part of obtained expression that is \n",
    "$$\n",
    "    \\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-y_{j}|,\n",
    "$$\n",
    "for every fixed $j \\in 1, \\dots, n$.\n",
    "We can see that this expression is similar to the expression of loss in the optimal case. So, in the case of random choise loss expectation is just averaging of all expressions $\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-y_{j}|$. But in the optimal case we obtained the loss value that is minimal among all $\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-y_{j}|$. Average value can not be less that minimal value (in the same set of values). <br>\n",
    "So, we proved that expected loss will be not less than the loss for optimal prediction.\n",
    "\n",
    "\n",
    "2. \n",
    "\n",
    "3. Let's find expected loss in the case of the random output of the label.\n",
    "$$\n",
    "    E(L^{*}) = \\frac{1}{n}\\sum_{j=1}^{n}\\frac{1}{n}\\sum_{i=1}^{n}\\lbrack y_{i} \\neq y_{j} \\rbrack = \\frac{1}{n}\\sum_{j=1}^{n}\\frac{1}{n}\\sum_{i=1}^{n} (1 - \\lbrack y_{i} = y_{j} \\rbrack) = \\frac{1}{n}\\bigg(\\sum_{j=1}^{n} \\big(1 - \\frac{1}{n}\\sum_{i=1}^{n}\\lbrack y_{i} = y_{j} \\rbrack\\big)\\bigg) = \\frac{1}{n}\\bigg(n - \\frac{1}{n}\\sum_{j=1}^{n}\\sum_{i=1}^{n}\\lbrack y_{i} = y_{j} \\rbrack\\bigg) = 1 - \\frac{1}{n}\n",
    "$$\n",
    "Now let's denote number of the most frequent label $\\hat{y}$ by $n'$. Then, similarly with the previous\n",
    "$$\n",
    "    L_{optimal} = 1 - \\frac{n'}{n}\n",
    "$$\n",
    "And we have that $E(L^{*}) \\ge L_{optimal}$, expected loss will be not less than the loss for optimal prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7*. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Define the fraction of concordant pairs by \n",
    "$$\\text{FCP}(f, X^{n})=\\frac{2}{n(n-1)}\\sum_{i<j}[y_{i}<=y_{j}].$$ Show that this value is equal to Area Under ROC of classifier $f$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernel Regression (1 point)\n",
    "Recall that the prediction of Kernel Ridge Regression fitted on data $X^{n}$ with the kernel $K(\\cdot, \\cdot)$ has the form $\\mathcal{K}(x)=\\sum_{i=1}^{n}\\alpha_{i}K(x, x_{i})$, where $\\alpha=(K+\\lambda I)^{-1}Y$ ($K_{ij}=K(x_{i},x_{j})$). The time complexity of computation of a prediction $\\mathcal{K}(x)$ for any point $x$ is $O(n)$, i.e. grows linearly with the size of the training set.\n",
    "\n",
    "Consider the bilinear kernel $K(x, x')=\\langle x, x'\\rangle$. For this kernel, the Kernel Regression is known to turn into simple linear ridge regression. However, for linear regression the computation time of prediction $\\mathcal{R}(x)=\\sum_{j=1}^{d}\\beta_{j}x^{j}$ is $O(d)$, where $d$ is the dimension of the feature space and does not grow with the training, which is a little bit controversary to the previous paragraph.\n",
    "\n",
    "In this task in order to show that the kernel regression with the bilinear kernel is indeed the linear ridge regression, you have to prove that the predictions exactly match by **direct comparison**.\n",
    "### Your solution:\n",
    "Let's consider prediction of simple linear ridge regression. $\\mathcal{R}(x)=\\sum_{j=1}^{d}\\omega_{j}x^{j}$. Weights $\\omega$ can be found from the optimizatons of $\\min\\limits_{\\omega} F(W) = \\|XW-Y\\|^2 + \\lambda\\|W\\|^2$ (Here $b$ assumed as zero as in lecture 2).\n",
    "$$\n",
    "    \\nabla F(W) = 2\\lambda W + 2X^\\top (XW-Y) = 0\n",
    "$$\n",
    "Note that here we consider feature vectors $x$ as a row vector (notation from lectures).\n",
    "$$\n",
    "    W = (X^\\top X + \\lambda I)^{-1} X^{\\top}Y\n",
    "$$\n",
    "So, prediction of linear ridge regression written in the form $\\mathcal{R}(x') = x'W = x'(X^\\top X + \\lambda I)^{-1} X^{\\top}Y$ <br>\n",
    "Now consider kernel ridge regression. $\\mathcal{K}(x)=\\sum_{i=1}^{n}\\alpha_{i}K(x, x_{i})$, where $\\alpha=(K+\\lambda I)^{-1}Y$, ($K_{ij}=K(x_{i},x_{j})$). If we use bilinear kernel $K(x, x')=\\langle x, x'\\rangle$ then kernel matrix represented as $K = \\lbrack x_1 \\dots x_m \\rbrack^\\top \\times \\lbrack x_1 \\dots x_m \\rbrack = XX^\\top$. So, $\\alpha = (XX^\\top + \\lambda I)^{-1}Y$. Prediciton of kernel ridge regression will be given as $\\mathcal{K}(x')=\\sum_{i=1}^{n}\\alpha_{i}K(x', x_{i}) = x' X^\\top \\alpha = x' X^\\top (XX^\\top + \\lambda I)^{-1}Y$.<br>\n",
    "At this point we have $\\mathcal{R}(x') = x'(X^\\top X + \\lambda I)^{-1} X^{\\top}Y$  and $\\mathcal{K}(x') = x' X^\\top (XX^\\top + \\lambda I)^{-1}Y$. <br>\n",
    "Let's prove that $X^\\top (XX^\\top + \\lambda I)^{-1} = (X^\\top X + \\lambda I)^{-1} X^{\\top}$. To solve it we need just to multiply valid equality $X^\\top (XX^\\top + \\lambda I) = (X^\\top X + \\lambda I) X^{\\top}$ from left by $(X^\\top X + \\lambda I)^{-1}$ and from right by $(XX^\\top + \\lambda I)^{-1}$, and it is proved.<br>\n",
    "After proving this equality we can show $\\mathcal{K}(x') = x'(X^\\top X + \\lambda I)^{-1} X^\\top Y = \\mathcal{R}(x')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that the function $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite kernel.\n",
    "### Your solution:\n",
    "Let's prove the statement.<br>\n",
    "$K(x,x')=\\exp(-\\|x-x'\\|^{2}) = \\exp(-\\|x\\|^2)\\exp(-\\|x'\\|^2)\\exp(2\\langle x^\\top x' \\rangle)$ <br>\n",
    "Let's denote $f(x) = \\exp(-\\|x\\|^2)$ and prove that $f(x)f(x')$ is a kernel, when $f(x): X \\to \\mathbb{R}$. <br>\n",
    "Indeed, Gramm matrix $K$ for $f(x)f(x')$ is $\\lbrack f(x_1) f(x_2) \\dots f(x_n)\\rbrack^\\top \\times \\lbrack f(x'_1) f(x'_2) \\dots f(x'_n)\\rbrack$ This matrix has rank 1. So, there is only one none zero eigenvalue. Since sum of eigenvalues equals the trace of the matrix, $\\lambda = \\sum f(x_i)f(x'_i)$. Consequently, lambda is just a scalar product of vectors. In addition, it is obvious that this matrix is symmetric (Gram matrix). From definition of Symmetric Positive Definite matrix from the lecture 2 we have that this matrix is SPSD matrix. So, $\\exp(-\\|x\\|^2)\\exp(-\\|x'\\|^2)$ is positive definite symmetric (PDS). From [Mercer’s Theorem](http://web.iitd.ac.in/~sumeet/CLT2008S-lecture18.pdf) get that $f(x)f(x') = \\exp(-\\|x\\|^2)\\exp(-\\|x'\\|^2)$ is a kernel. <br>\n",
    "In a similar way we can prove that  $\\exp(2\\langle x^\\top x' \\rangle)$ is a kernel. Here in the exponent we have $\\langle x^\\top x' \\rangle \\ge 0$ and symmetric. Using the property of a kernel that $\\exp(k(x,x'))$ is a kernel, where $k(x,x')$ is a kernel (this property is proved in several lines, for example, [here](https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/8.pdf), page 3).<br>\n",
    "Now we have a product of two kernels which is also the kernel, according to the properties of a kernel function ([here](https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/8.pdf)). <br>\n",
    "So, we proved that $K(x,x')=\\exp(-\\|x-x'\\|^{2}) = \\exp(-\\|x\\|^2)\\exp(-\\|x'\\|^2)\\exp(2\\langle x^\\top x' \\rangle)$ is a kernel function. $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is a symmetric function. So, according to [Mercer’s Theorem](http://web.iitd.ac.in/~sumeet/CLT2008S-lecture18.pdf), kernel matrix of $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive semi-definite. It means (according to the leccture 2) that the kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is PDS (positive definite symmetric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10*. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
