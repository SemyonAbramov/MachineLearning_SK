{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 3: Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem just right after the words **BEGIN SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "* If you want an easy life, you have to use **BUILT-IN METHODS** of `sklearn` library instead of writing tons of our yown code. There exists a class/method for almost everything you can imagine (related to this homework).\n",
    "* To do some tasks in this part of homework, you have to write **CODE** directly inside specified places inside notebook **CELLS**.\n",
    "* In some problems you may be asked to provide short discussion of the results. In this cases you have to create **MARKDOWN** cell with your comments right after the your code cell.\n",
    "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if the reviewer decides to execute `Kernel` -> `Restart Kernel and Run All Cells`, after all the computation he will obtain exactly the same solution (with all the corresponding plots) as in your uploaded notebook. For this purpose, we suggest to fix random `seed` or (better) define `random_state=` inside every algorithm that uses some pseudorandomness.\n",
    "\n",
    "* Your code must be clear to the reviewer. For this purpose, try to include neccessary comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY** without any additional comments.\n",
    "* The are problems with \\* mark - they are not obligatory. You can get **EXTRA POINTS** for solving them.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write your implementation within the designated blocks:\n",
    "```python\n",
    "...\n",
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "\n",
    "### END Solution\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Models. GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (1 pt.)\n",
    "\n",
    "Consider a univariate Gaussian distribution $\\mathcal{N}(x; \\mu, \\tau^{-1})$.\n",
    "Let's define Gaussian-Gamma prior for parameters $(\\mu, \\tau)$:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\mu, \\tau)\n",
    "        = \\mathcal{N}(\\mu; \\mu_0, (\\beta \\tau)^{-1})\n",
    "            \\otimes \\text{Gamma}(\\tau; a, b)\n",
    "        \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Find the posterior distribution of $(\\mu, \\tau)$ after observing $X = (x_1, \\dots, x_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**<br>\n",
    "Let's consider given Gaussian-Gamma distribution $p(\\mu, \\tau) = \\mathcal{N}(\\mu; \\mu_0, (\\beta \\tau)^{-1}) \\otimes \\text{Gamma}(\\tau; a, b)$, where\n",
    "\n",
    "$$\n",
    "    \\text{Gamma}(\\tau; a, b) = \\dfrac{b^{a}}{\\Gamma(a)}\\tau^{a - 1}\\exp(-\\tau b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    N(\\mu; \\mu_{0},(\\beta \\tau)^{-1}) = \\dfrac{1}{(2\\pi (\\beta \\tau)^{-1})^{\\frac{1}{2}}}\\exp\\big(-\\dfrac{1}{2(\\beta \\tau)^{-1}}(\\mu - \\mu_{0})^{2}\\big)\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "    p(\\mu, \\tau) = \\mathcal{N}(\\mu; \\mu_0, (\\beta \\tau)^{-1}) \\otimes \\text{Gamma}(\\tau; a, b) = \\dfrac{b^{a}}{\\Gamma(a)}\\tau^{a - \\frac{1}{2}} \\dfrac{\\beta^{\\frac{1}{2}}}{(2\\pi)^{\\frac{1}{2}}} \\exp(-\\tau b) \\exp\\big(-\\dfrac{1}{2(\\beta \\tau)^{-1}}(\\mu - \\mu_{0})^{2}\\big)\n",
    "$$\n",
    "\n",
    "For posterior distribution we have posterior ~ likelihood $\\times$ prior\n",
    "\n",
    "$$\n",
    "    p(\\mu, \\tau \\vert X) \\sim p(X \\vert \\mu, \\tau)p(\\mu, \\tau) \\space \\space \\space \\text{(posterior ~ likelihood $\\times$ prior)}\n",
    "$$\n",
    "\n",
    "Likelihood is given by\n",
    "$$\n",
    "    p(X \\vert \\mu, \\tau) = \\prod_{i = 1}^{n} \\mathcal{N}(X \\vert \\mu, \\tau^{-1}) = \\dfrac{1}{(2\\pi)^{\\frac{n}{2}}}\\tau^{\\frac{n}{2}}\\exp\\big(-\\dfrac{\\tau}{2}\\sum_{i = 1}^{n}(x_{i} - \\mu)^{2}\\big)\n",
    "$$\n",
    "\n",
    "Using notation $\\bar{x} = \\dfrac{\\sum_{i = 1}^{n}x_{i}}{n}$, we will have\n",
    "\n",
    "$$\n",
    "    p(\\mu, \\tau \\vert X) \\sim \\dfrac{b^{a}}{\\Gamma(a)}\\tau^{a - \\frac{1}{2}} \\dfrac{\\beta^{\\frac{1}{2}}}{(2\\pi)^{\\frac{1}{2}}} \\exp(-\\tau b) \\exp\\big(-\\dfrac{1}{2(\\beta \\tau)^{-1}}(\\mu - \\mu_{0})^{2}\\big) \\dfrac{1}{(2\\pi)^{\\frac{n}{2}}}\\tau^{\\frac{n}{2}}\\exp\\big(-\\dfrac{\\tau}{2}\\sum_{i = 1}^{n}(x_{i} - \\mu)^{2}\\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p(\\mu, \\tau \\vert X) \\sim \\dfrac{b^{a}}{\\Gamma(a)}\\tau^{a - \\frac{1}{2}} \\dfrac{\\beta^{\\frac{1}{2}}}{(2\\pi)^{\\frac{1}{2}}} \\dfrac{1}{(2\\pi)^{\\frac{n}{2}}}\\tau^{\\frac{n}{2}} \\exp(-\\tau b)\\exp\\big(-\\dfrac{1}{2(\\beta \\tau)^{-1}}(\\mu - \\mu_{0})^{2}\\big) \\exp\\big(-\\dfrac{\\tau}{2}\\sum_{i = 1}^{n}(x_{i} - \\mu)^{2}\\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p(\\mu, \\tau \\vert X) \\sim \\dfrac{b^{a}}{\\Gamma(a)}\\tau^{\\frac{n}{2} + a - \\frac{1}{2}} \\dfrac{\\beta^{\\frac{1}{2}}}{(2\\pi)^{\\big(\\frac{1}{2} + \\frac{n}{2}\\big)}} \\exp(-\\tau b)\\exp\\big(-\\dfrac{\\beta \\tau}{2}(\\mu - \\mu_{0})^{2}\\big) \\exp\\big(-\\dfrac{\\tau}{2}\\big(n(\\mu - \\bar{x})^{2} + \\sum_{i = 1}^{n}(x_{i} - x)^{2}\\big)\\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p(\\mu, \\tau \\vert X) \\sim \\dfrac{b^{a}}{\\Gamma(a)}\\tau^{\\frac{n}{2} + a - \\frac{1}{2}} \\dfrac{\\beta^{\\frac{1}{2}}}{(2\\pi)^{\\big(\\frac{1}{2} + \\frac{n}{2}\\big)}} \\exp(-\\tau b)\\exp\\big(\\dfrac{\\tau}{2}\\big(\\beta(\\mu - \\mu_{0})^{2} +n(\\mu - \\bar{x})^{2} + \\sum_{i = 1}^{n}(x_{i} - x)^{2}\\big)\\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p(\\mu, \\tau \\vert X) \\sim \\dfrac{b^{a}}{\\Gamma(a)}\\tau^{\\frac{n}{2} + a - \\frac{1}{2}} \\dfrac{\\beta^{\\frac{1}{2}}}{(2\\pi)^{\\big(\\frac{1}{2} + \\frac{n}{2}\\big)}} \\exp(-\\tau b)\\exp\\big(\\dfrac{\\tau}{2}\\big( (\\beta + n)\\big(\\mu - \\dfrac{\\beta \\mu_{0} + n\\bar{x}}{\\beta + n}\\big)^{2} + \\dfrac{\\beta n (\\bar{x} - \\mu_{0})^{2}}{\\beta + n} + \\sum_{i = 1}^{n}(x_{i} - x)^{2}\\big)\\big)\n",
    "$$\n",
    "\n",
    "So posterior distribution is expressed as,\n",
    "\n",
    "$$\n",
    "    p(\\mu, \\tau \\vert X) \\sim \\mathcal{N}\\big(\\mu; \\dfrac{\\beta \\mu_{0} + n\\bar{x}}{\\beta + n}, ((\\beta + n)\\tau)^{-1}\\big) \\otimes \\text{Gamma}\\big(\\tau; a + \\dfrac{n}{2}, b + \\dfrac{\\beta n (\\bar{x} - \\mu_{0})^{2}}{2(\\beta + n)} + \\dfrac{1}{2}\\sum_{i = 1}^{n}(x_{i} - x)^{2}\\big)\n",
    "$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (1 + 1 + 1 = 3 pt.)\n",
    "\n",
    "Evaluate the following integral using the Laplace approximation:\n",
    "\\begin{equation}\n",
    "    x \\mapsto \\int \\sigma(w^T x) \\mathcal{N}(w; 0, \\Sigma) dw \\,,\n",
    "\\end{equation}\n",
    "for $x = \\bigl(\\tfrac23, \\tfrac16, \\tfrac16\\bigr)\\in \\mathbb{R}^3$ and \n",
    "\\begin{equation}\n",
    "    \\Sigma\n",
    "        = \\begin{pmatrix}\n",
    "             1    & -0.25 & 0.75 \\\\\n",
    "            -0.25 &  1    & 0.5  \\\\\n",
    "             0.75 &  0.5  & 2\n",
    "           \\end{pmatrix}\n",
    "        \\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (1 pt.)\n",
    "Use the Hessian matrix computed numericaly via finite differences. (Check out [Numdifftools](https://pypi.python.org/pypi/Numdifftools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use Laplace approximation as described by Bishop in \"Pattern Recognition and Machine Learning\", chapter 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "def dfunc(w):\n",
    "    x = np.array([2/3, 1/6, 1/6])\n",
    "    cov_matrix = np.array([[1.0, -0.25, 0.75], [-0.25, 1.0, 0.5], [0.75, 0.5, 2.0]])\n",
    "    w = w.astype(dtype = np.float64)\n",
    "    \n",
    "    sigma = 1 / (1 + np.exp(-w.T @ x))\n",
    "    gaussian_distr = (1 / (2 * np.pi)**(3/2)) * (1 / (np.linalg.det(cov_matrix))**(1/2)) *  np.exp(-0.5 * w.T @ np.linalg.inv(cov_matrix) @ w)\n",
    "    \n",
    "    return sigma * gaussian_distr\n",
    "\n",
    "def minus_ln_dfunc(w):\n",
    "    return -np.log(dfunc(w))\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of integral:  0.49791972092509307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numdifftools/limits.py:182: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  f_del = np.vstack(list(np.ravel(r)) for r in sequence)\n",
      "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numdifftools/limits.py:184: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for step in steps)\n",
      "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#x = np.array([2/3, 1/6, 1/6])\n",
    "#cov_matrix = np.array([[1, -0.25, 0.75], [-0.25, 1, 0.5], [0.75, 0.5, 2]])\n",
    "\n",
    "w_init = np.array([0.1, 0.3, 0.4])\n",
    "w_optimal = minimize(minus_ln_dfunc, x0=w_init).x\n",
    "\n",
    "H = nd.Hessian(minus_ln_dfunc)(w_optimal)\n",
    "\n",
    "norm_integral = dfunc(w_optimal) * (2 * np.pi)**(3/2) / (np.linalg.det(H))**(1/2)\n",
    "\n",
    "print (\"The value of integral: \", norm_integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 (1 pt.)\n",
    "Use the diagonal approximation of the Hessian computed by autodifferentiation\n",
    "in **pytorch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "def dfunc_torch(w):\n",
    "    \n",
    "    x = np.array([2/3, 1/6, 1/6])\n",
    "    x = Variable(torch.from_numpy(x).type(torch.FloatTensor))\n",
    "    cov_matrix = np.array([[1.0, -0.25, 0.75], [-0.25, 1.0, 0.5], [0.75, 0.5, 2.0]])\n",
    "    \n",
    "    inv_cov_matr = Variable(torch.from_numpy(np.linalg.inv(cov_matrix)).type(torch.FloatTensor))\n",
    "    gauss_distr = (1/((2*np.pi)**(3/2)*np.linalg.det(cov_matrix)**(1/2))) * torch.exp(-0.5 * w @ inv_cov_matr @ w)\n",
    "    \n",
    "    return gauss_distr * torch.sigmoid(torch.dot(w, x))\n",
    "    \n",
    "def minus_ln_dfunc_torch(w):\n",
    "    return -torch.log(dfunc_torch(w))\n",
    "\n",
    "def hessian_diag_approx(f, w):\n",
    "    z = Variable(torch.FloatTensor(w), requires_grad=True)\n",
    "    dim = z.size()[0]\n",
    "    \n",
    "    y = f(z)\n",
    "    first_grad = torch.autograd.grad(y, z, create_graph=True)[0]\n",
    "        \n",
    "    hessian = []\n",
    "    for i in range (dim):\n",
    "        hessian.append(torch.autograd.grad(first_grad[i], z, create_graph=True)[0].data.numpy())\n",
    "    \n",
    "    return np.diag(np.diag(hessian))\n",
    "\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of integral:  0.29643095\n"
     ]
    }
   ],
   "source": [
    "H = hessian_diag_approx(minus_ln_dfunc_torch, w_optimal)\n",
    "\n",
    "torch_norm_integral = dfunc_torch(Variable(torch.from_numpy(w_optimal).type(torch.FloatTensor))) * (2*np.pi)**(3/2) / np.linalg.det(H)**(1/2)\n",
    "torch_norm_integral = torch_norm_integral.data.numpy()\n",
    "\n",
    "print('The value of integral: ', torch_norm_integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 (1 pt.)\n",
    "\n",
    "Compare the results comparing the `absolute errors` of the results (this is possible with Monte-Carlo estimate of the integral). Write 1-2 sentences in the results discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (1 + 2 = 3 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 (1 pt.)\n",
    "\n",
    "Assuimng the matrices $A \\in \\mathbb{R}^{n \\times n}$ and $D \\in \\mathbb{R}^{d \\times d}$\n",
    "are invertible, using **gaussian elimination** find the inverse matrix for the following\n",
    "block matrix:\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix} A & B \\\\ C & D \\end{pmatrix} \\,,\n",
    "\\end{equation}\n",
    "where $C \\in \\mathbb{R}^{d \\times n}$ and $B \\in \\mathbb{R}^{n \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 (2 pt.)\n",
    "\n",
    "Assume that the function $y(x)$, $x \\in \\mathbb{R}^d$, is a realization of the Gaussian\n",
    "Process $GP\\bigl(0; K(\\cdot, \\cdot)\\bigr)$ with $K(a, b) = \\exp({- \\gamma \\|a - b\\|_2^2}))$.\n",
    "\n",
    "Suppose two datasets were observed: **noiseless** ${D_0}$ and **noisy** ${D_1}$\n",
    "\\begin{aligned}\n",
    "    & D_0 = \\bigl(x_i, y(x_i) \\bigr)_{i=1}^{n} \\,, \\\\\n",
    "    & D_1 = \\bigl(x^\\prime_i, y(x^\\prime_i) + \\varepsilon_i \\bigr)_{i=1}^{m} \\,,\n",
    "\\end{aligned}\n",
    "where $\\varepsilon_i \\sim \\text{ iid } \\mathcal{N}(0, \\sigma^2)$, independent of process $y$.\n",
    "\n",
    "Derive the conditional distribution of $y(x) \\big\\vert_{D_0, D_1}$ at a new $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (1 + 2 = 3 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1 (1 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the late 1950’s Charles Keeling invented an accurate way to measure atmospheric $CO_2$ concentration and began taking regular measurements at the Mauna Loa observatory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take `monthly_co2_mlo.csv` file, load it and prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Load the `CO2 [ppm]` time series\n",
    "1. Replace $-99.99$ with **NaN** and drop the missing observations\n",
    "2. Split the time series into train and test\n",
    "3. Normalize the target value by fitting a transformation on the train\n",
    "4. Plot the resulting target against the time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> your code here <<<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2 (2 pt.)\n",
    "\n",
    "Use [**GPy**](https://pypi.python.org/pypi/GPy) library for training and prediction. Fit a GP and run the predict on the test. Useful kernels to combine: `GPy.kern.RBF, GPy.kern.Poly, GPy.kern.StdPeriodic, GPy.kern.White, GPy.kern.Linear`. \n",
    "\n",
    "1. Plot mean and confidence interval of the prediction. \n",
    "2. Inspect them on normality by scatter plot: plot predicted points/time series against true values. \n",
    "3. Estimate the prediction error with `r2_score`. R2-score accepted > 0.83 on test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPy.models import GPRegression\n",
    "from GPy.kern import RBF, Poly, StdPeriodic, White, Linear\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> your code here <<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(# >>> your code here <<<)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
